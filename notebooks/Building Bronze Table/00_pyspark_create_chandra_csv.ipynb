{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d2b587-5d08-4e28-b4e5-70edee893c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config with iterative process for each csv file in the list\n",
    "\n",
    "# === CONFIG UMUM ===\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG        = \"lapse_scoring_dev\"\n",
    "SCHEMA_LANDING = \"00_landing\"\n",
    "SCHEMA_BRONZE  = \"01_bronze\"\n",
    "VOLUME_NAME    = \"chandra\"\n",
    "WRITE_MODE     = \"append\"   # atau \"overwrite\" untuk refresh awal\n",
    "\n",
    "BASE_DIR = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA_LANDING}/{VOLUME_NAME}\"\n",
    "\n",
    "# Daftar pekerjaan: (SRC_PATTERN relatif terhadap BASE_DIR) -> TABLE_NAME di Bronze\n",
    "JOBS = [\n",
    "    {\"SRC_PATTERN\": \"201811/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201811_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201811/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201811_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201812/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201812_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201812/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201812_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201901/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201901_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201901/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201901_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201902/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201902_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201902/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201902_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201903/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201903_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201903/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201903_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201904/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201904_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201904/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201904_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201905/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201905_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201905/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201905_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201906/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201906_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201906/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201906_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201907/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201907_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201907/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201907_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201908/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201908_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201908/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201908_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201909/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201909_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201909/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201909_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201910/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201910_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201910/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201910_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201911/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201911_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201911/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201911_ul_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201912/TRAD_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201912_trad_master_bronze\"},\n",
    "    {\"SRC_PATTERN\": \"201912/UL_MASTER.csv\",          \"TABLE_NAME\": \"chandra_201912_ul_master_bronze\"},\n",
    "    # Tambah baris lain di sini...\n",
    "]\n",
    "\n",
    "# === PERSIAPAN CATALOG/SCHEMA BRONZE ===\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{SCHEMA_BRONZE}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA_BRONZE}`\")\n",
    "\n",
    "def detect_sep(sample_path: str) -> str:\n",
    "    \"\"\"Deteksi delimiter dari header file (`,` vs `;`).\"\"\"\n",
    "    head = dbutils.fs.head(sample_path, 4096)\n",
    "    return \",\" if head.count(\",\") >= head.count(\";\") else \";\"\n",
    "\n",
    "def ingest_one(src_pattern: str, table_name: str):\n",
    "    src_path = f\"{BASE_DIR}/{src_pattern}\"\n",
    "    target_table = f'{CATALOG}.`{SCHEMA_BRONZE}`.{table_name}'\n",
    "    print(f\"\\n==> Ingest: {src_path} -> {target_table}\")\n",
    "\n",
    "    # Deteksi delimiter\n",
    "    sep = detect_sep(src_path)\n",
    "    print(f\"Detected delimiter: '{sep}'\")\n",
    "\n",
    "    # Baca CSV\n",
    "    df = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"sep\", sep)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .option(\"quote\", '\"')\n",
    "          .option(\"escape\", '\"')\n",
    "          .option(\"multiLine\", True)\n",
    "          .option(\"mode\", \"PERMISSIVE\")\n",
    "          .csv(src_path))\n",
    "\n",
    "    # Normalisasi ringan nama kolom\n",
    "    df = df.toDF(*[c.strip().lower().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "    # Kolom ID -> string (kalau ada)\n",
    "    for c in [\"chdrnum\", \"clntnum\", \"lifenum\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "    # Metadata UC-friendly\n",
    "    df = (df\n",
    "          .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "          .withColumn(\"_source_path\", F.col(\"_metadata.file_path\")))\n",
    "\n",
    "    # Tulis ke Bronze\n",
    "    (df.write\n",
    "        .mode(WRITE_MODE)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(target_table))\n",
    "\n",
    "    # Verifikasi ringan\n",
    "    cnt = spark.table(target_table).count()\n",
    "    print(f\"Write OK → {target_table} | rows={cnt}\")\n",
    "\n",
    "# === JALANKAN SEMUA JOB ===\n",
    "errors = []\n",
    "for job in JOBS:\n",
    "    try:\n",
    "        ingest_one(job[\"SRC_PATTERN\"], job[\"TABLE_NAME\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {job['SRC_PATTERN']} -> {job['TABLE_NAME']} :: {e}\")\n",
    "        errors.append((job, str(e)))\n",
    "\n",
    "if errors:\n",
    "    print(\"\\n=== SUMMARY: SOME JOBS FAILED ===\")\n",
    "    for (job, msg) in errors:\n",
    "        print(f\"- {job['SRC_PATTERN']} -> {job['TABLE_NAME']} :: {msg}\")\n",
    "else:\n",
    "    print(\"\\n=== ALL JOBS COMPLETED SUCCESSFULLY ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9f0c7b-9632-41e5-b6b5-74ab7d486a2e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762065845693}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== STEP 1: konfigurasi ====\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG        = \"lapse_scoring_dev\"\n",
    "SCHEMA_LANDING = \"00_landing\"\n",
    "SCHEMA_BRONZE  = \"01_bronze\"\n",
    "VOLUME_NAME    = \"chandra\"   # folder di Volume\n",
    "SRC_PATTERN    = \"201803/TRAD_MASTER 201803.csv\"  # pola file CSV di landing\n",
    "TABLE_NAME     = \"chandra_201803_trad_master_bronze\"\n",
    "WRITE_MODE     = \"append\"    # \"overwrite\" untuk full refresh awal\n",
    "\n",
    "SRC_PATH = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA_LANDING}/{VOLUME_NAME}/{SRC_PATTERN}\"\n",
    "TARGET_TABLE = f'{CATALOG}.`{SCHEMA_BRONZE}`.{TABLE_NAME}'\n",
    "\n",
    "# ==== STEP 2: pastikan catalog & schema bronze ====\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{SCHEMA_BRONZE}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA_BRONZE}`\")\n",
    "\n",
    "# ==== STEP 3: READ CSV dari landing ====\n",
    "\n",
    "# 2) deteksi delimiter dari header\n",
    "header = dbutils.fs.head(SRC_PATH, 4096)\n",
    "comma_ct = header.count(\",\")\n",
    "semi_ct  = header.count(\";\")\n",
    "sep = \",\" if comma_ct >= semi_ct else \";\"\n",
    "\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .option(\"sep\", sep)\n",
    "      # sesuaikan delimiter jika perlu: .option(\"sep\",\";\")\n",
    "      .option(\"multiLine\", True)   # jika ada kolom teks panjang dengan newline\n",
    "      .option(\"quote\", '\"')\n",
    "      .option(\"escape\", '\"')\n",
    "      .option(\"mode\", \"PERMISSIVE\")\n",
    "      .csv(SRC_PATH))\n",
    "\n",
    "# Normalisasi ringan nama kolom\n",
    "df = df.toDF(*[c.strip().lower().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "# Kolom ID (policy/client/life) amankan sebagai STRING\n",
    "for c in [\"chdrnum\", \"clntnum\", \"lifenum\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "# Tambahkan metadata teknis\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "      .withColumn(\"_source_path\", F.col(\"_metadata.file_path\"))  # ← pengganti input_file_name()\n",
    ")\n",
    "\n",
    "# ==== STEP 4: TULIS ke Bronze (Delta) ====\n",
    "(df.write\n",
    "   .mode(WRITE_MODE)\n",
    "   .option(\"mergeSchema\",\"true\")\n",
    "   .format(\"delta\")\n",
    "   .saveAsTable(TARGET_TABLE))\n",
    "\n",
    "print(f\"Write OK → {TARGET_TABLE}\")\n",
    "\n",
    "# ==== STEP 5: verifikasi cepat ====\n",
    "print(\"Row count:\", spark.table(TARGET_TABLE).count())\n",
    "display(spark.sql(f\"SELECT * FROM {TARGET_TABLE} LIMIT 20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec9dc54-1c89-4829-9565-a07ef9ecfe43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== STEP 1: konfigurasi ====\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG        = \"lapse_scoring_dev\"\n",
    "SCHEMA_LANDING = \"00_landing\"\n",
    "SCHEMA_BRONZE  = \"01_bronze\"\n",
    "VOLUME_NAME    = \"chandra\"   # folder di Volume\n",
    "SRC_PATTERN    = \"201712/TRAD_MASTER 201712.csv\"  # pola file CSV di landing\n",
    "TABLE_NAME     = \"chandra_201712_trad_master_bronze\"\n",
    "WRITE_MODE     = \"append\"    # \"overwrite\" untuk full refresh awal\n",
    "\n",
    "SRC_PATH = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA_LANDING}/{VOLUME_NAME}/{SRC_PATTERN}\"\n",
    "TARGET_TABLE = f'{CATALOG}.`{SCHEMA_BRONZE}`.{TABLE_NAME}'\n",
    "\n",
    "print(SRC_PATH);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33742c94-8bc4-49cb-983f-bfad8e77cffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== STEP 2: pastikan catalog & schema bronze ====\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{SCHEMA_BRONZE}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA_BRONZE}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a4f785-feae-4e89-bd70-224c538564ab",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762026651178}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== STEP 3: READ CSV dari landing ====\n",
    "\n",
    "# 2) deteksi delimiter dari header\n",
    "header = dbutils.fs.head(SRC_PATH, 4096)\n",
    "comma_ct = header.count(\",\")\n",
    "semi_ct  = header.count(\";\")\n",
    "sep = \",\" if comma_ct >= semi_ct else \";\"\n",
    "\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .option(\"sep\", sep)\n",
    "      # sesuaikan delimiter jika perlu: .option(\"sep\",\";\")\n",
    "      .option(\"multiLine\", True)   # jika ada kolom teks panjang dengan newline\n",
    "      .option(\"quote\", '\"')\n",
    "      .option(\"escape\", '\"')\n",
    "      .option(\"mode\", \"PERMISSIVE\")\n",
    "      .csv(SRC_PATH))\n",
    "\n",
    "# Normalisasi ringan nama kolom\n",
    "df = df.toDF(*[c.strip().lower().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "# Kolom ID (policy/client/life) amankan sebagai STRING\n",
    "for c in [\"chdrnum\", \"clntnum\", \"lifenum\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "# Tambahkan metadata teknis\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "      .withColumn(\"_source_path\", F.col(\"_metadata.file_path\"))  # ← pengganti input_file_name()\n",
    ")\n",
    "\n",
    "# (opsional) intip hasil\n",
    "df.printSchema()\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a25185a-bb31-4df9-ad9c-0c81821cafca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== STEP 4: TULIS ke Bronze (Delta) ====\n",
    "(df.write\n",
    "   .mode(WRITE_MODE)\n",
    "   .option(\"mergeSchema\",\"true\")\n",
    "   .format(\"delta\")\n",
    "   .saveAsTable(TARGET_TABLE))\n",
    "\n",
    "print(f\"Write OK → {TARGET_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c954520-4834-4700-8419-4ff16d5aa4c9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762029957176}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== STEP 5: verifikasi cepat ====\n",
    "print(\"Row count:\", spark.table(TARGET_TABLE).count())\n",
    "display(spark.sql(f\"SELECT * FROM {TARGET_TABLE} LIMIT 20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "865a5347-1686-4c47-8205-da1b2d3b2be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_pyspark_create_chandra_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
