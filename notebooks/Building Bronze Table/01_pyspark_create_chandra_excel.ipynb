{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818692f6-5805-4d96-bc2e-b10076c8a048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install dependency untuk read_excel\n",
    "%pip install openpyxl\n",
    "\n",
    "# (opsional di beberapa runtime) restart kernel python agar paket terdeteksi\n",
    "import sys\n",
    "if \"databricks\" in sys.version.lower():\n",
    "    try:\n",
    "        dbutils.library.restartPython()\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617694a8-3688-4eff-a6a5-954d11f281b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf66bfda-f130-46f9-85f9-35d5130848f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1 — konfigurasi\n",
    "import pandas as pd\n",
    "import glob, os, datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "CATALOG = \"lapse_scoring_dev\"\n",
    "SCHEMA_LANDING = \"00_landing\"\n",
    "SCHEMA_BRONZE  = \"01_bronze\"\n",
    "VOLUME_NAME    = \"chandra\"    # ubah jika nama volumenya berbeda\n",
    "#TABLE_NAME     = \"chandra_201703_trad_master_bronze\"    # nama tabel bronze tujuan\n",
    "TABLE_NAME     = \"chandra_201701_trad_master_bronze\"    # nama tabel bronze tujuan\n",
    "WRITE_MODE     = \"overwrite\"           # ganti \"overwrite\" untuk run pertama jika mau reset; \"append\" untuk tambah\n",
    "\n",
    "# Path: pandas harus pakai /Volumes, Spark boleh dbfs:/Volumes\n",
    "LANDING_DIR_PANDAS = f\"/Volumes/{CATALOG}/{SCHEMA_LANDING}/{VOLUME_NAME}\"\n",
    "LANDING_XLSX_GLOB  = os.path.join(LANDING_DIR_PANDAS, \"201703/TRAD_MASTER.xlsx\")\n",
    "TARGET_TABLE       = f'{CATALOG}.`{SCHEMA_BRONZE}`.{TABLE_NAME}'\n",
    "\n",
    "# STEP 2 — pastikan catalog & schema bronze ada\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{SCHEMA_BRONZE}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA_BRONZE}`\")\n",
    "\n",
    "# STEP 3 — helper: normalisasi kolom & baca semua sheet dari 1 file\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns={c: c.strip().lower().replace(\" \", \"_\") for c in df.columns})\n",
    "\n",
    "def read_all_sheets_with_meta(xlsx_path: str) -> list[pd.DataFrame]:\n",
    "    # sheet_name=None -> dict {sheet: DataFrame}\n",
    "    sheets = pd.read_excel(xlsx_path, sheet_name=None)\n",
    "    out = []\n",
    "    for sheet_name, pdf in sheets.items():\n",
    "        if pdf is None or len(pdf) == 0:\n",
    "            continue\n",
    "        pdf = normalize_cols(pdf)\n",
    "        # tambah metadata sumber & sheet\n",
    "        pdf[\"_source_path\"] = xlsx_path\n",
    "        pdf[\"_sheet_name\"]  = str(sheet_name)\n",
    "        out.append(pdf)\n",
    "    return out\n",
    "\n",
    "# STEP 4 — kumpulkan semua file .xlsx (rekursif) & baca\n",
    "xlsx_files = glob.glob(LANDING_XLSX_GLOB, recursive=True)\n",
    "if not xlsx_files:\n",
    "    raise FileNotFoundError(f\"Tidak ditemukan file .xlsx di {LANDING_DIR_PANDAS}. Upload dulu file Excel ke volume tersebut.\")\n",
    "\n",
    "pdf_all_list = []\n",
    "for f in xlsx_files:\n",
    "    try:\n",
    "        pdf_all_list.extend(read_all_sheets_with_meta(f))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Gagal baca: {f} -> {e}\")\n",
    "\n",
    "if not pdf_all_list:\n",
    "    raise ValueError(\"Tidak ada sheet yang terbaca dari file-file Excel.\")\n",
    "\n",
    "# gabungkan semua sheet dari semua file\n",
    "pdf_all = pd.concat(pdf_all_list, ignore_index=True)\n",
    "\n",
    "# STEP 5 — convert ke Spark DataFrame & tambah _ingest_ts (UPDATED)\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pdf = pdf_all.copy()\n",
    "\n",
    "# --- A) Sanitizer nama kolom (Delta/UC safe) ---\n",
    "def sanitize(name: str) -> str:\n",
    "    if name is None:\n",
    "        name = \"\"\n",
    "    n = str(name)\n",
    "\n",
    "    # jika kolom metadata internal (mis. _source_path, _sheet_name), biarkan apa adanya\n",
    "    if n.startswith(\"_\"):\n",
    "        return n\n",
    "\n",
    "    # trim + lowercase\n",
    "    n = n.strip().lower()\n",
    "    # ganti karakter terlarang dengan underscore\n",
    "    n = re.sub(r\"[ ,;{}()\\n\\t=]+\", \"_\", n)\n",
    "    # ganti semua non-alfanumerik (selain underscore) dengan underscore\n",
    "    n = re.sub(r\"[^a-z0-9_]\", \"_\", n)\n",
    "    # kompres underscore berulang\n",
    "    n = re.sub(r\"_+\", \"_\", n)\n",
    "    # hapus underscore di awal/akhir\n",
    "    n = n.strip(\"_\")\n",
    "    # jika kosong, beri nama default\n",
    "    if n == \"\":\n",
    "        n = \"col\"\n",
    "    # jika diawali angka, tambahkan prefix agar valid\n",
    "    if re.match(r\"^[0-9]\", n):\n",
    "        n = f\"c_{n}\"\n",
    "    return n\n",
    "\n",
    "# terapkan sanitizer + jaga keunikan\n",
    "safe_cols = []\n",
    "seen = {}\n",
    "for c in pdf.columns:\n",
    "    base = sanitize(c)\n",
    "    alias = base\n",
    "    k = 1\n",
    "    while alias in seen:\n",
    "        k += 1\n",
    "        alias = f\"{base}_{k}\"\n",
    "    seen[alias] = True\n",
    "    safe_cols.append(alias)\n",
    "\n",
    "pdf.columns = safe_cols\n",
    "\n",
    "# --- B) Pastikan ID/nomor polis bertipe string (hapus akhiran .0 hasil Excel) ---\n",
    "for c in [\"chdrnum\", \"clntnum\", \"lifenum\"]:\n",
    "    if c in pdf.columns:\n",
    "        pdf[c] = (pdf[c].astype(\"string\")\n",
    "                          .str.replace(r\"\\.0$\", \"\", regex=True))\n",
    "\n",
    "# --- C) Cast object campuran ke string; biarkan numerik/datetime apa adanya ---\n",
    "for c in pdf.columns:\n",
    "    if pdf[c].dtype == \"object\":\n",
    "        pdf[c] = pdf[c].astype(\"string\")\n",
    "\n",
    "# --- D) Ganti NaN/NaT jadi None agar Spark tidak error saat Arrow conversion ---\n",
    "pdf = pdf.where(pd.notnull(pdf), None)\n",
    "\n",
    "# --- E) Buat Spark DF + metadata ---\n",
    "df = spark.createDataFrame(pdf)\n",
    "df = df.withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "# STEP 6 — tulis ke Bronze (Delta), mode append, allow schema evolve\n",
    "(\n",
    "    df.write\n",
    "      .mode(WRITE_MODE)                 # \"append\" (default), atau \"overwrite\" untuk reset awal\n",
    "      .option(\"mergeSchema\", \"true\")    # kalau ada kolom baru saat run berikutnya\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(TARGET_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"Write OK → {TARGET_TABLE}\")\n",
    "\n",
    "# STEP 7 — verifikasi cepat\n",
    "cnt = spark.table(TARGET_TABLE).count()\n",
    "print(\"Row count:\", cnt)\n",
    "display(spark.sql(f\"SELECT * FROM {TARGET_TABLE} LIMIT 20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55cace04-fba7-4b4d-abba-305c2e13326a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2 — pastikan catalog & schema bronze ada\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{SCHEMA_BRONZE}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA_BRONZE}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee9c483-67fc-46cb-a3fa-596e8cb11c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 3 — helper: normalisasi kolom & baca semua sheet dari 1 file\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns={c: c.strip().lower().replace(\" \", \"_\") for c in df.columns})\n",
    "\n",
    "def read_all_sheets_with_meta(xlsx_path: str) -> list[pd.DataFrame]:\n",
    "    # sheet_name=None -> dict {sheet: DataFrame}\n",
    "    sheets = pd.read_excel(xlsx_path, sheet_name=None)\n",
    "    out = []\n",
    "    for sheet_name, pdf in sheets.items():\n",
    "        if pdf is None or len(pdf) == 0:\n",
    "            continue\n",
    "        pdf = normalize_cols(pdf)\n",
    "        # tambah metadata sumber & sheet\n",
    "        pdf[\"_source_path\"] = xlsx_path\n",
    "        pdf[\"_sheet_name\"]  = str(sheet_name)\n",
    "        out.append(pdf)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0fefab-01ca-4083-abe8-22d69ea73596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4 — kumpulkan semua file .xlsx (rekursif) & baca\n",
    "xlsx_files = glob.glob(LANDING_XLSX_GLOB, recursive=True)\n",
    "if not xlsx_files:\n",
    "    raise FileNotFoundError(f\"Tidak ditemukan file .xlsx di {LANDING_DIR_PANDAS}. Upload dulu file Excel ke volume tersebut.\")\n",
    "\n",
    "pdf_all_list = []\n",
    "for f in xlsx_files:\n",
    "    try:\n",
    "        pdf_all_list.extend(read_all_sheets_with_meta(f))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Gagal baca: {f} -> {e}\")\n",
    "\n",
    "if not pdf_all_list:\n",
    "    raise ValueError(\"Tidak ada sheet yang terbaca dari file-file Excel.\")\n",
    "\n",
    "# gabungkan semua sheet dari semua file\n",
    "pdf_all = pd.concat(pdf_all_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb50bfd4-f73d-4a3b-9ee7-1fc4a2fb3487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5 — convert ke Spark DataFrame & tambah _ingest_ts (UPDATED)\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pdf = pdf_all.copy()\n",
    "\n",
    "# --- A) Sanitizer nama kolom (Delta/UC safe) ---\n",
    "def sanitize(name: str) -> str:\n",
    "    if name is None:\n",
    "        name = \"\"\n",
    "    n = str(name)\n",
    "\n",
    "    # jika kolom metadata internal (mis. _source_path, _sheet_name), biarkan apa adanya\n",
    "    if n.startswith(\"_\"):\n",
    "        return n\n",
    "\n",
    "    # trim + lowercase\n",
    "    n = n.strip().lower()\n",
    "    # ganti karakter terlarang dengan underscore\n",
    "    n = re.sub(r\"[ ,;{}()\\n\\t=]+\", \"_\", n)\n",
    "    # ganti semua non-alfanumerik (selain underscore) dengan underscore\n",
    "    n = re.sub(r\"[^a-z0-9_]\", \"_\", n)\n",
    "    # kompres underscore berulang\n",
    "    n = re.sub(r\"_+\", \"_\", n)\n",
    "    # hapus underscore di awal/akhir\n",
    "    n = n.strip(\"_\")\n",
    "    # jika kosong, beri nama default\n",
    "    if n == \"\":\n",
    "        n = \"col\"\n",
    "    # jika diawali angka, tambahkan prefix agar valid\n",
    "    if re.match(r\"^[0-9]\", n):\n",
    "        n = f\"c_{n}\"\n",
    "    return n\n",
    "\n",
    "# terapkan sanitizer + jaga keunikan\n",
    "safe_cols = []\n",
    "seen = {}\n",
    "for c in pdf.columns:\n",
    "    base = sanitize(c)\n",
    "    alias = base\n",
    "    k = 1\n",
    "    while alias in seen:\n",
    "        k += 1\n",
    "        alias = f\"{base}_{k}\"\n",
    "    seen[alias] = True\n",
    "    safe_cols.append(alias)\n",
    "\n",
    "pdf.columns = safe_cols\n",
    "\n",
    "# --- B) Pastikan ID/nomor polis bertipe string (hapus akhiran .0 hasil Excel) ---\n",
    "for c in [\"chdrnum\", \"clntnum\", \"lifenum\"]:\n",
    "    if c in pdf.columns:\n",
    "        pdf[c] = (pdf[c].astype(\"string\")\n",
    "                          .str.replace(r\"\\.0$\", \"\", regex=True))\n",
    "\n",
    "# --- C) Cast object campuran ke string; biarkan numerik/datetime apa adanya ---\n",
    "for c in pdf.columns:\n",
    "    if pdf[c].dtype == \"object\":\n",
    "        pdf[c] = pdf[c].astype(\"string\")\n",
    "\n",
    "# --- D) Ganti NaN/NaT jadi None agar Spark tidak error saat Arrow conversion ---\n",
    "pdf = pdf.where(pd.notnull(pdf), None)\n",
    "\n",
    "# --- E) Buat Spark DF + metadata ---\n",
    "df = spark.createDataFrame(pdf)\n",
    "df = df.withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153c7353-a099-4cae-aac2-fca79db7b9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 6 — tulis ke Bronze (Delta), mode append, allow schema evolve\n",
    "(\n",
    "    df.write\n",
    "      .mode(WRITE_MODE)                 # \"append\" (default), atau \"overwrite\" untuk reset awal\n",
    "      .option(\"mergeSchema\", \"true\")    # kalau ada kolom baru saat run berikutnya\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(TARGET_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"Write OK → {TARGET_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2097a2b-4f17-43fb-9bef-db9019f5ebae",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762027284216}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 7 — verifikasi cepat\n",
    "cnt = spark.table(TARGET_TABLE).count()\n",
    "print(\"Row count:\", cnt)\n",
    "display(spark.sql(f\"SELECT * FROM {TARGET_TABLE} LIMIT 20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85bae05a-4b69-41d2-a817-caffaeea9633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_pyspark_create_chandra_excel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
