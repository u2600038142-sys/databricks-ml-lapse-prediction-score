{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786ca4ee-7593-45e0-b497-286d0a134d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config with iterative process for each csv file in the list\n",
    "\n",
    "# === CONFIG UMUM ===\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG        = \"lapse_scoring_dev\"\n",
    "SCHEMA_LANDING = \"00_landing\"\n",
    "SCHEMA_BRONZE  = \"01_bronze\"\n",
    "VOLUME_NAME    = \"fwd_max_daily\"\n",
    "WRITE_MODE     = \"append\"   # atau \"overwrite\" untuk refresh awal\n",
    "\n",
    "BASE_DIR = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA_LANDING}/{VOLUME_NAME}\"\n",
    "\n",
    "# Daftar pekerjaan: (SRC_PATTERN relatif terhadap BASE_DIR) -> TABLE_NAME di Bronze\n",
    "JOBS = [\n",
    "    {\"SRC_PATTERN\": \"Master_category_interest.csv\",          \"TABLE_NAME\": \"fwd_max_daily_master_category_interest\"},\n",
    "    {\"SRC_PATTERN\": \"fwd_max_member.csv\",          \"TABLE_NAME\": \"fwd_max_daily_fwd_max_member\"},\n",
    "    {\"SRC_PATTERN\": \"fwd_max_transaction.csv\",          \"TABLE_NAME\": \"fwd_max_daily_fwd_max_transaction\"},\n",
    "    # Tambah baris lain di sini...\n",
    "]\n",
    "\n",
    "# === PERSIAPAN CATALOG/SCHEMA BRONZE ===\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{SCHEMA_BRONZE}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA_BRONZE}`\")\n",
    "\n",
    "def detect_sep(sample_path: str) -> str:\n",
    "    \"\"\"Deteksi delimiter dari header file (`,` vs `;`).\"\"\"\n",
    "    head = dbutils.fs.head(sample_path, 4096)\n",
    "    return \",\" if head.count(\",\") >= head.count(\";\") else \";\"\n",
    "\n",
    "def ingest_one(src_pattern: str, table_name: str):\n",
    "    src_path = f\"{BASE_DIR}/{src_pattern}\"\n",
    "    target_table = f'{CATALOG}.`{SCHEMA_BRONZE}`.{table_name}'\n",
    "    print(f\"\\n==> Ingest: {src_path} -> {target_table}\")\n",
    "\n",
    "    # Deteksi delimiter\n",
    "    sep = detect_sep(src_path)\n",
    "    print(f\"Detected delimiter: '{sep}'\")\n",
    "\n",
    "    # Baca CSV\n",
    "    df = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"sep\", sep)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .option(\"quote\", '\"')\n",
    "          .option(\"escape\", '\"')\n",
    "          .option(\"multiLine\", True)\n",
    "          .option(\"mode\", \"PERMISSIVE\")\n",
    "          .csv(src_path))\n",
    "\n",
    "    # Normalisasi ringan nama kolom\n",
    "    df = df.toDF(*[c.strip().lower().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "    # Kolom ID -> string (kalau ada)\n",
    "    for c in [\"chdrnum\", \"clntnum\", \"lifenum\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "    # Metadata UC-friendly\n",
    "    df = (df\n",
    "          .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "          .withColumn(\"_source_path\", F.col(\"_metadata.file_path\")))\n",
    "\n",
    "    # Tulis ke Bronze\n",
    "    (df.write\n",
    "        .mode(WRITE_MODE)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(target_table))\n",
    "\n",
    "    # Verifikasi ringan\n",
    "    cnt = spark.table(target_table).count()\n",
    "    print(f\"Write OK â†’ {target_table} | rows={cnt}\")\n",
    "\n",
    "# === JALANKAN SEMUA JOB ===\n",
    "errors = []\n",
    "for job in JOBS:\n",
    "    try:\n",
    "        ingest_one(job[\"SRC_PATTERN\"], job[\"TABLE_NAME\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {job['SRC_PATTERN']} -> {job['TABLE_NAME']} :: {e}\")\n",
    "        errors.append((job, str(e)))\n",
    "\n",
    "if errors:\n",
    "    print(\"\\n=== SUMMARY: SOME JOBS FAILED ===\")\n",
    "    for (job, msg) in errors:\n",
    "        print(f\"- {job['SRC_PATTERN']} -> {job['TABLE_NAME']} :: {msg}\")\n",
    "else:\n",
    "    print(\"\\n=== ALL JOBS COMPLETED SUCCESSFULLY ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310fde48-2ddd-4eaf-ad91-8ba31bdff61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_pyspark_create_fwdmaxdaily_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
