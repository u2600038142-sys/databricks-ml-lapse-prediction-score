{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07b0638-e971-4c42-b548-5d9e7e74cd30",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762558805370}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# UNION Bronze -> Silver (handle dots in column names)\n",
    "# =========================\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# ===== CONFIG =====\n",
    "CATALOG    = \"lapse_scoring_dev\"\n",
    "SRC_SCHEMA = \"01_bronze\"\n",
    "DST_SCHEMA = \"02_silver\"\n",
    "\n",
    "TABLES = [\n",
    "    \"fwd_max_daily_fwd_max_transaction\",\n",
    "]\n",
    "\n",
    "# Kolom asli di Bronze (raw) — mengandung titik\n",
    "WANTED_RAW = [\n",
    "    \"no.\",\"trans_id\",\"member_id\",\"client_code\",\"fwd_max_card_no.\",\"name\",\"member_type\",\"member_status\",\"act_code\",\"activity\",\"policy_no\",\"point\",\"create_date\",\"ods_trans_id\",\"reward_code\",\"info\",\"status\",\"redeem_date\", \"alacard_ds_date\", \"alacard_req._id\"\n",
    "]\n",
    "\n",
    "TARGET_TABLE = \"fwd_max_transaction_silver\"\n",
    "WRITE_MODE   = \"overwrite\"\n",
    "\n",
    "# ===== Helpers =====\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Bikin nama kolom aman: lowercase + ganti selain [a-z0-9_] jadi underscore + trim underscore.\"\"\"\n",
    "    n = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", s).lower()\n",
    "    n = re.sub(r\"_+\", \"_\", n).strip(\"_\")\n",
    "    if not n or n[0].isdigit():\n",
    "        n = f\"c_{n}\" if n else \"col\"\n",
    "    return n\n",
    "\n",
    "# Map raw->safe (contoh: \"no.\" -> \"no\", \"policy_no.\" -> \"policy_no\")\n",
    "RAW2SAFE = {raw: safe_name(raw) for raw in WANTED_RAW}\n",
    "\n",
    "def select_wanted_with_alias(df, raw2safe):\n",
    "    \"\"\"Select kolom raw (pakai backtick) dan alias ke nama safe; yang tidak ada -> NULL.\"\"\"\n",
    "    cols_df = set(df.columns)\n",
    "    sel = []\n",
    "    for raw, safe in raw2safe.items():\n",
    "        if raw in cols_df:\n",
    "            sel.append(F.col(f\"`{raw}`\").cast(\"string\").alias(safe))  # backtick untuk nama bertitik\n",
    "        else:\n",
    "            sel.append(F.lit(None).cast(\"string\").alias(safe))\n",
    "    return df.select(*sel)\n",
    "\n",
    "def union_all(dfs):\n",
    "    return reduce(lambda a,b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "# ===== Ensure destination schema exists =====\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.`{DST_SCHEMA}`\")\n",
    "\n",
    "# ===== Load, align, and union =====\n",
    "dfs = []\n",
    "for t in TABLES:\n",
    "    src_fqn = f\"{CATALOG}.`{SRC_SCHEMA}`.{t}\"\n",
    "    df0 = spark.table(src_fqn)\n",
    "\n",
    "    dfi = select_wanted_with_alias(df0, RAW2SAFE) \\\n",
    "            .withColumn(\"_source_table\", F.lit(t)) \\\n",
    "            .withColumn(\"_ingest_ts_union\", F.current_timestamp())\n",
    "\n",
    "    dfs.append(dfi)\n",
    "\n",
    "df_union = dfs[0] if len(dfs) == 1 else union_all(dfs)\n",
    "\n",
    "# ===== Write to Silver =====\n",
    "TARGET_FQN = f\"{CATALOG}.`{DST_SCHEMA}`.{TARGET_TABLE}\"\n",
    "(\n",
    "    df_union.write\n",
    "      .mode(WRITE_MODE)\n",
    "      .option(\"mergeSchema\",\"true\")\n",
    "      .option(\"overwriteSchema\",\"true\")\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(TARGET_FQN)\n",
    ")\n",
    "\n",
    "print(f\"Write OK -> {TARGET_FQN}\")\n",
    "print(\"Rows:\", spark.table(TARGET_FQN).count())\n",
    "display(spark.sql(f\"SELECT * FROM {TARGET_FQN} LIMIT 50\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec804f0-73c0-46e6-a335-fc775dff5a7e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762555075453}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Rename columns to business names (make a new Silver table)\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG     = \"lapse_scoring_dev\"\n",
    "SRC_SCHEMA  = \"02_silver\"\n",
    "SRC_TABLE   = \"fwd_max_transaction_silver\"          # tabel hasil union (semua STRING)\n",
    "DST_SCHEMA  = \"02_silver\"\n",
    "DST_TABLE   = \"fwd_max_transaction_bn_silver\"       # bn = business names\n",
    "\n",
    "# Mapping: old -> new (edit jika perlu)\n",
    "BUSINESS_MAP = {\n",
    "    \"no\":   \"Row_No\",\n",
    "    \"trans_id\": \"Transaction_ID\",\n",
    "    \"member_id\": \"Member_ID\",\n",
    "    \"client_code\": \"Client_Number\",\n",
    "    \"fwd_max_card_no\": \"FWD_Max_Card_No\",\n",
    "    \"name\": \"Member_Name\",\n",
    "    \"member_type\": \"Member_Type\",\n",
    "    \"member_status\": \"Member Status\",\n",
    "    \"act_code\": \"Activity Code\",\n",
    "    \"activity\": \"Activity Name\",\n",
    "    \"policy_no\": \"Contract_Number\",\n",
    "    \"point\": \"Point_Value\",\n",
    "    \"create_date\": \"Create_Date\",\n",
    "    \"ods_trans_id\": \"ODS_Transaction_ID\",\n",
    "    \"reward_code\": \"Reward_Code\",\n",
    "    \"info\": \"Transaction_Info\",\n",
    "    \"status\": \"Transaction_Status\",\n",
    "    \"redeem_date\": \"Redeem_Date\",\n",
    "    \"alacard_ds_date\": \"Alacard_Send_Date\",\n",
    "    \"alacard_req_id\": \"Alacard_Request_ID\",\n",
    "    # lineage\n",
    "    \"_source_table\":     \"source_table\",\n",
    "    \"_ingest_ts_union\":  \"union_ingest_ts\",\n",
    "}\n",
    "\n",
    "SRC_FQN = f\"{CATALOG}.`{SRC_SCHEMA}`.{SRC_TABLE}\"\n",
    "DST_FQN = f\"{CATALOG}.`{DST_SCHEMA}`.{DST_TABLE}\"\n",
    "\n",
    "df = spark.table(SRC_FQN)\n",
    "\n",
    "# cek kolom yang ada vs mapping\n",
    "existing = set(df.columns)\n",
    "mapped_existing = {k:v for k,v in BUSINESS_MAP.items() if k in existing}\n",
    "missing_in_source = [k for k in BUSINESS_MAP.keys() if k not in existing]\n",
    "if missing_in_source:\n",
    "    print(\"⚠️ Missing in source (ignored):\", missing_in_source)\n",
    "\n",
    "# select dengan alias business\n",
    "df_bn = df.select([F.col(k).alias(v.replace(' ', '_').replace('-', '_').replace('.', '_')) for k,v in mapped_existing.items()])\n",
    "\n",
    "# tulis sebagai tabel baru\n",
    "(df_bn.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\",\"true\")\n",
    "    .option(\"overwriteSchema\",\"true\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(DST_FQN))\n",
    "\n",
    "print(f\"Write OK -> {DST_FQN}\")\n",
    "display(spark.sql(f\"SELECT * FROM {DST_FQN} LIMIT 50\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f511446-2f38-4be8-8a59-042d4c68fd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CAST tipe data dari BN -> simpan ke tabel BARU\n",
    "# ============================================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- CONFIG ---\n",
    "CATALOG      = \"lapse_scoring_dev\"\n",
    "SRC_SCHEMA   = \"02_silver\"\n",
    "SRC_TABLE    = \"fwd_max_transaction_bn_silver\"        # tabel BN sumber (string-heavy)\n",
    "DST_SCHEMA   = \"02_silver\"\n",
    "DST_TABLE    = \"fwd_max_transaction_dt_silver\"  # >>> tabel BARU hasil casting\n",
    "\n",
    "SRC_FQN = f\"{CATALOG}.`{SRC_SCHEMA}`.{SRC_TABLE}\"\n",
    "DST_FQN = f\"{CATALOG}.`{DST_SCHEMA}`.{DST_TABLE}\"\n",
    "\n",
    "# Pastikan schema tujuan ada\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.`{DST_SCHEMA}`\")\n",
    "\n",
    "# --- LOAD ---\n",
    "df = spark.table(SRC_FQN)\n",
    "\n",
    "# 1) Trim semua kolom string\n",
    "for c, t in df.dtypes:\n",
    "    if t.startswith(\"string\"):\n",
    "        df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "# 2) Uppercase untuk kode/flag (sesuaikan jika ada tambahan)\n",
    "uppercase_cols = [\n",
    "    \"Member_Type\",\n",
    "]\n",
    "for c in uppercase_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.upper(F.col(c)))\n",
    "\n",
    "# 3) Parse TANGGAL/WAKTU: \"9/20/2017 11:02:56 AM\" (M/d/yyyy h:mm:ss a)\n",
    "ts_cols = [\"Create_Date\", \"Redeem_Date\", \"Alacard_Send_Date\"]\n",
    "\n",
    "def parse_us_ampm(colname: str):\n",
    "    # Variasi pola untuk tanggal 1/2 digit & jam 1/2 digit\n",
    "    return F.coalesce(\n",
    "        F.expr(f\"try_to_timestamp({colname}, 'M/d/yyyy h:mm:ss a')\"),\n",
    "        F.expr(f\"try_to_timestamp({colname}, 'MM/dd/yyyy h:mm:ss a')\"),\n",
    "        F.expr(f\"try_to_timestamp({colname}, 'M/d/yyyy hh:mm:ss a')\"),\n",
    "        F.expr(f\"try_to_timestamp({colname}, 'MM/dd/yyyy hh:mm:ss a')\")\n",
    "    )\n",
    "\n",
    "for c in ts_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, parse_us_ampm(c))\n",
    "\n",
    "# 4) Numerik (double): bersihkan locale lalu cast ke double\n",
    "def to_double(col):\n",
    "    s = F.regexp_replace(col.cast(\"string\"), r\"\\s\", \"\")\n",
    "    return (\n",
    "        F.when(s.rlike(r\"^\\d{1,3}(\\.\\d{3})+,\\d+$\"), F.regexp_replace(F.regexp_replace(s, r\"\\.\", \"\"), \",\", \".\"))\n",
    "         .when(s.rlike(r\"^\\d+,\\d+$\"),               F.regexp_replace(s, \",\", \".\"))\n",
    "         .when(s.rlike(r\"^\\d{1,3}(,\\d{3})+\\.\\d+$\"), F.regexp_replace(s, \",\", \"\"))\n",
    "         .otherwise(s)\n",
    "    ).cast(\"double\")\n",
    "\n",
    "double_cols = [\n",
    "    # Tambahkan kolom numeric lain jika ada, mis.:\n",
    "    # \"Loan_Value\", \"Total_Fee\", \"Coverage_Debt\"\n",
    "]\n",
    "for c in double_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, to_double(F.col(c)))\n",
    "\n",
    "# 5) Integer: usia (Life_Number dibiarkan string jika itu ID)\n",
    "int_cols = [\n",
    "    \"Row_No\",\n",
    "    \"Transaction_ID\",\n",
    "    \"Member_ID\",\"Member_Status\",\"Point_Value\",\"ODS_Transaction_ID\",\"Transaction_Status\",\n",
    "]\n",
    "for c in int_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(\"int\"))\n",
    "\n",
    "# 6) Pastikan ID/teks utama tetap STRING (defensif)\n",
    "id_string_cols = [\n",
    "    \"FWD_Max_Card_No\",\"Client_Number\", \"Member_Name\",\"Activity_Code\",\"Activity_Name\", \"Contract_Number\", \"Reward_Code\", \"Transaction_Info\",\n",
    "]\n",
    "for c in id_string_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "# 7) Tulis ke TABEL BARU (tidak menyentuh tabel sumber)\n",
    "(df.write\n",
    "    .mode(\"overwrite\")              # buat/replace tabel baru\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(DST_FQN))\n",
    "\n",
    "print(f\"Typed table written -> {DST_FQN}\")\n",
    "print(\"Rows:\", spark.table(DST_FQN).count())\n",
    "display(spark.sql(f\"DESCRIBE {DST_FQN}\"))\n",
    "display(spark.sql(f\"SELECT * FROM {DST_FQN} LIMIT 20\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bfc9f93-3e13-49b4-b150-0731393baf4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_pyspark_for_fwd_max_transaction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
