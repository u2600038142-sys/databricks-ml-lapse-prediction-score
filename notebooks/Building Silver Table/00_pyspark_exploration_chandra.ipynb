{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac023aa0-6753-497d-8f1f-58ace7e44e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tujuan: Mencari kolom yang akan dipertahankan\n",
    "#Jawaban: chdrnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9defe5f3-ef4d-4472-9a72-4ec94afdde01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPEN CHANDRA METADATA\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from functools import reduce\n",
    "\n",
    "# ===== CONFIG =====\n",
    "CATALOG = \"lapse_scoring_dev\"\n",
    "SCHEMA  = \"01_bronze\"\n",
    "\n",
    "# pilih salah satu:\n",
    "TABLES = [\n",
    "    \"chandra_metadata_bronze\",\n",
    "    # \"chandra_201702_trad_master_bronze\",\n",
    "    # \"pak_tani_golden_customers_360\",\n",
    "]\n",
    "# atau pakai pola:\n",
    "TABLE_PATTERN = None  # mis. \"chandra_%_trad_master_bronze\" (None = tidak dipakai)\n",
    "\n",
    "def list_tables(catalog, schema, pattern=None):\n",
    "    if pattern:\n",
    "        q = f\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM {catalog}.information_schema.tables\n",
    "        WHERE table_schema = '{schema}'\n",
    "          AND table_type = 'BASE TABLE'\n",
    "          AND table_name LIKE '{pattern}'\n",
    "        \"\"\"\n",
    "        return [r[\"table_name\"] for r in spark.sql(q).collect()]\n",
    "    return TABLES\n",
    "\n",
    "def explore_table(fullname):\n",
    "    print(f\"\\n=== EDA: {fullname} ===\")\n",
    "    df = spark.table(fullname)\n",
    "\n",
    "    # ---- ringkasan cepat\n",
    "    rows = df.count()\n",
    "    print(\"Rows:\", rows)\n",
    "    display(df.limit(100))\n",
    "    display(spark.createDataFrame(df.dtypes, [\"column\",\"type\"]))\n",
    "    print(\"\\n=== END EDA ===\")\n",
    "\n",
    "tables = list_tables(CATALOG, \"01_bronze\", pattern=TABLE_PATTERN) if TABLE_PATTERN else TABLES\n",
    "for t in tables:\n",
    "    explore_table(f\"{CATALOG}.`{SCHEMA}`.{t}\")\n",
    "\n",
    "#keep chdrnum, shortdesc, aracde, cnttype, crtable, hissdte, subdat, payfreq, sumins, statcode, lifenum, age02, dob02, gender, smokeind, annprem01, annprem02, ptdate, btdate, clntnum, age01, dob01, pldesc01, pldesc02, prat, orig_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b20dfe7-0d3a-4b27-a9c7-91d129394b4d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762470009119}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# EDA Presence Check (Bronze) + Row Count\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ===== CONFIG =====\n",
    "CATALOG = \"lapse_scoring_dev\"\n",
    "SCHEMA  = \"01_bronze\"\n",
    "\n",
    "# Daftar tabel (atau pakai TABLE_PATTERN)\n",
    "TABLES = [\n",
    "    \"chandra_201701_trad_master_bronze\",\n",
    "    \"chandra_201701_ul_master_bronze\",\n",
    "    \"chandra_201702_trad_master_bronze\",\n",
    "    \"chandra_201702_ul_master_bronze\",\n",
    "    \"chandra_201703_trad_master_bronze\",\n",
    "    \"chandra_201703_ul_master_bronze\",\n",
    "    \"chandra_201704_trad_master_bronze\",\n",
    "    \"chandra_201704_ul_master_bronze\",\n",
    "    \"chandra_201705_trad_master_bronze\",\n",
    "    \"chandra_201705_ul_master_bronze\",\n",
    "    \"chandra_201706_trad_master_bronze\",\n",
    "    \"chandra_201706_ul_master_bronze\",\n",
    "    \"chandra_201707_trad_master_bronze\",\n",
    "    \"chandra_201707_ul_master_bronze\",\n",
    "    \"chandra_201708_trad_master_bronze\",\n",
    "    \"chandra_201708_ul_master_bronze\",\n",
    "    \"chandra_201709_trad_master_bronze\",\n",
    "    \"chandra_201709_ul_master_bronze\",\n",
    "    \"chandra_201710_trad_master_bronze\",\n",
    "    \"chandra_201710_ul_master_bronze\",\n",
    "    \"chandra_201711_trad_master_bronze\",\n",
    "    \"chandra_201711_ul_master_bronze\",\n",
    "    \"chandra_201712_trad_master_bronze\",\n",
    "    \"chandra_201712_ul_master_bronze\",\n",
    "    \"chandra_201801_trad_master_bronze\",\n",
    "    \"chandra_201801_ul_master_bronze\",\n",
    "    \"chandra_201802_trad_master_bronze\",\n",
    "    \"chandra_201802_ul_master_bronze\",\n",
    "    \"chandra_201803_trad_master_bronze\",\n",
    "    \"chandra_201803_ul_master_bronze\",\n",
    "    \"chandra_201804_trad_master_bronze\",\n",
    "    \"chandra_201804_ul_master_bronze\",\n",
    "    \"chandra_201805_trad_master_bronze\",\n",
    "    \"chandra_201805_ul_master_bronze\",\n",
    "    \"chandra_201806_trad_master_bronze\",\n",
    "    \"chandra_201806_ul_master_bronze\",\n",
    "    \"chandra_201807_trad_master_bronze\",\n",
    "    \"chandra_201807_ul_master_bronze\",\n",
    "    \"chandra_201808_trad_master_bronze\",\n",
    "    \"chandra_201808_ul_master_bronze\",\n",
    "    \"chandra_201809_trad_master_bronze\",\n",
    "    \"chandra_201809_ul_master_bronze\",\n",
    "    \"chandra_201810_trad_master_bronze\",\n",
    "    \"chandra_201810_ul_master_bronze\",\n",
    "    \"chandra_201811_trad_master_bronze\",\n",
    "    \"chandra_201811_ul_master_bronze\",\n",
    "    \"chandra_201812_trad_master_bronze\",\n",
    "    \"chandra_201812_ul_master_bronze\",\n",
    "    \"chandra_201901_trad_master_bronze\",\n",
    "    \"chandra_201901_ul_master_bronze\",\n",
    "    \"chandra_201902_trad_master_bronze\",\n",
    "    \"chandra_201902_ul_master_bronze\",\n",
    "    \"chandra_201903_trad_master_bronze\",\n",
    "    \"chandra_201903_ul_master_bronze\",\n",
    "    \"chandra_201904_trad_master_bronze\",\n",
    "    \"chandra_201904_ul_master_bronze\",\n",
    "    \"chandra_201905_trad_master_bronze\",\n",
    "    \"chandra_201905_ul_master_bronze\",\n",
    "    \"chandra_201906_trad_master_bronze\",\n",
    "    \"chandra_201906_ul_master_bronze\",\n",
    "    \"chandra_201907_trad_master_bronze\",\n",
    "    \"chandra_201907_ul_master_bronze\",\n",
    "    \"chandra_201908_trad_master_bronze\",\n",
    "    \"chandra_201908_ul_master_bronze\",\n",
    "    \"chandra_201909_trad_master_bronze\",\n",
    "    \"chandra_201909_ul_master_bronze\",\n",
    "    \"chandra_201910_trad_master_bronze\",\n",
    "    \"chandra_201910_ul_master_bronze\",\n",
    "    \"chandra_201911_trad_master_bronze\",\n",
    "    \"chandra_201911_ul_master_bronze\",\n",
    "    \"chandra_201912_trad_master_bronze\",\n",
    "    \"chandra_201912_ul_master_bronze\",\n",
    "]\n",
    "TABLE_PATTERN = None  # contoh: \"chandra_%_trad_master_bronze\"\n",
    "\n",
    "# Kolom yang ingin dipastikan ada/seragam\n",
    "WANTED = [\n",
    "    \"chdrnum\",\"shortdesc\",\"aracde\",\"cnttype\",\"crtable\",\"hissdte\",\"subdat\",\n",
    "    \"payfreq\",\"sumins\",\"statcode\",\"lifenum\",\"age02\",\"dob02\",\"gender\",\n",
    "    \"smokeind\",\"annprem01\",\"ptdate\",\"btdate\",\"clntnum\",\n",
    "    \"age01\",\"dob01\",\"pldesc01\",\"pldesc02\"\n",
    "]\n",
    "\n",
    "# ===== Utils =====\n",
    "def list_tables(catalog, schema, pattern=None):\n",
    "    if pattern:\n",
    "        q = f\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM {catalog}.information_schema.tables\n",
    "        WHERE table_schema = '{schema}'\n",
    "          AND table_type = 'BASE TABLE'\n",
    "          AND table_name LIKE '{pattern}'\n",
    "        \"\"\"\n",
    "        return [r[\"table_name\"] for r in spark.sql(q).collect()]\n",
    "    return TABLES\n",
    "\n",
    "def build_presence_report(catalog, schema, tables, wanted):\n",
    "    rows = []\n",
    "    wanted_set = set(wanted)\n",
    "    for t in tables:\n",
    "        fullname = f\"{catalog}.`{schema}`.{t}\"\n",
    "        df = spark.table(fullname)\n",
    "        cols = set(df.columns)\n",
    "        missing = sorted(list(wanted_set - cols))\n",
    "        extra   = sorted(list(cols - wanted_set))\n",
    "        row_cnt = df.count()  # row count\n",
    "\n",
    "        rows.append({\n",
    "            \"table_name\": t,\n",
    "            \"row_count\": row_cnt,\n",
    "            \"wanted_count\": len(wanted),\n",
    "            \"present_count\": len(wanted_set & cols),\n",
    "            \"missing_count\": len(missing),\n",
    "            \"missing_cols\": \", \".join(missing) if missing else \"\",\n",
    "            \"extra_count\": len(extra),\n",
    "            \"extra_sample\": \", \".join(extra[:10])\n",
    "        })\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "def build_column_matrix(catalog, schema, tables, wanted):\n",
    "    rows = []\n",
    "    for t in tables:\n",
    "        fullname = f\"{catalog}.`{schema}`.{t}\"\n",
    "        cols = set(spark.table(fullname).columns)\n",
    "        for w in wanted:\n",
    "            rows.append({\"table_name\": t, \"column\": w, \"present\": int(w in cols)})\n",
    "    df = spark.createDataFrame(rows)\n",
    "    return (df.groupBy(\"table_name\")\n",
    "              .pivot(\"column\", wanted)\n",
    "              .agg(F.max(\"present\"))\n",
    "              .orderBy(\"table_name\"))\n",
    "\n",
    "def select_with_missing_as_null(df, wanted, default_type=\"string\"):\n",
    "    cols = df.columns\n",
    "    selected = []\n",
    "    for c in wanted:\n",
    "        if c in cols:\n",
    "            selected.append(F.col(c))\n",
    "        else:\n",
    "            selected.append(F.lit(None).cast(default_type).alias(c))\n",
    "    return df.select(*selected)  # urutan sesuai WANTED\n",
    "\n",
    "def explore_table_selected(fullname: str):\n",
    "    print(f\"\\n=== EDA (selected cols): {fullname} ===\")\n",
    "    df = spark.table(fullname)\n",
    "\n",
    "    existing = [c for c in WANTED if c in df.columns]\n",
    "    missing  = [c for c in WANTED if c not in df.columns]\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing columns ({len(missing)}): {missing}\")\n",
    "    else:\n",
    "        print(\"All WANTED columns are present âœ…\")\n",
    "\n",
    "    # versi NULL-fill agar konsisten & siap union\n",
    "    df_sel = select_with_missing_as_null(df, WANTED)\n",
    "\n",
    "    # tampilkan contoh & schema\n",
    "    display(df_sel.limit(100))\n",
    "    display(spark.createDataFrame(df_sel.dtypes, [\"column\",\"type\"]))\n",
    "    print(\"=== END EDA ===\")\n",
    "\n",
    "# ===== Run presence checks =====\n",
    "tables = list_tables(CATALOG, SCHEMA, pattern=TABLE_PATTERN) if TABLE_PATTERN else TABLES\n",
    "\n",
    "presence = build_presence_report(CATALOG, SCHEMA, tables, WANTED)\n",
    "display(presence.orderBy(F.desc(\"missing_count\")))\n",
    "\n",
    "matrix = build_column_matrix(CATALOG, SCHEMA, tables, WANTED)\n",
    "\n",
    "# Tambahkan row_count ke matrix (kolom pertama)\n",
    "counts_df = spark.createDataFrame(\n",
    "    [(t, spark.table(f\"{CATALOG}.`{SCHEMA}`.{t}\").count()) for t in tables],\n",
    "    [\"table_name\", \"row_count\"]\n",
    ")\n",
    "matrix_with_counts = (matrix.join(counts_df, on=\"table_name\", how=\"left\")\n",
    "                             .select(\"table_name\",\"row_count\", *[c for c in matrix.columns if c != \"table_name\"])\n",
    "                             .orderBy(\"table_name\"))\n",
    "\n",
    "display(matrix_with_counts)\n",
    "\n",
    "# (Opsional) simpan laporan ke Silver untuk dokumentasi/audit\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.`02_silver`\")\n",
    "presence.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.`02_silver`.bronze_wanted_presence\")\n",
    "matrix_with_counts.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.`02_silver`.bronze_wanted_matrix\")\n",
    "\n",
    "# ===== Preview per tabel =====\n",
    "for t in tables:\n",
    "    explore_table_selected(f\"{CATALOG}.`{SCHEMA}`.{t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5997a1b6-eef1-4aa8-99fd-79938aac6357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===== CONFIG =====\n",
    "CATALOG = \"lapse_scoring_dev\"\n",
    "SCHEMA  = \"01_bronze\"\n",
    "\n",
    "# pilih salah satu:\n",
    "TABLES = [\n",
    "    \"chandra_201701_trad_master_bronze\",\n",
    "    # \"chandra_201702_trad_master_bronze\",\n",
    "    # \"pak_tani_golden_customers_360\",\n",
    "]\n",
    "# atau pakai pola:\n",
    "TABLE_PATTERN = None  # mis. \"chandra_%_trad_master_bronze\" (None = tidak dipakai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500fcec1-421f-43d7-9020-acd5462aa69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from functools import reduce\n",
    "\n",
    "def list_tables(catalog, schema, pattern=None):\n",
    "    if pattern:\n",
    "        q = f\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM {catalog}.information_schema.tables\n",
    "        WHERE table_schema = '{schema}'\n",
    "          AND table_type = 'BASE TABLE'\n",
    "          AND table_name LIKE '{pattern}'\n",
    "        \"\"\"\n",
    "        return [r[\"table_name\"] for r in spark.sql(q).collect()]\n",
    "    return TABLES\n",
    "\n",
    "def is_num(dt):   return dt.startswith(\"int\") or dt.startswith(\"bigint\") or dt.startswith(\"double\") or dt.startswith(\"float\") or dt.startswith(\"decimal\")\n",
    "def is_date(dt):  return dt in (\"date\",\"timestamp\")\n",
    "def is_str(dt):   return dt.startswith(\"string\") or dt.startswith(\"varchar\") or dt.startswith(\"char\")\n",
    "\n",
    "def topk(df, col, k=10):\n",
    "    return (df.groupBy(col).count().orderBy(F.desc(\"count\")).limit(k))\n",
    "\n",
    "def profile_numeric(df, col):\n",
    "    return (df.select(\n",
    "        F.count(F.col(col)).alias(\"non_null\"),\n",
    "        F.count(F.when(F.col(col).isNull(), col)).alias(\"nulls\"),\n",
    "        F.approx_count_distinct(col).alias(\"distinct_approx\"),\n",
    "        F.mean(col).alias(\"mean\"),\n",
    "        F.stddev(col).alias(\"stddev\"),\n",
    "        F.min(col).alias(\"min\"),\n",
    "        F.max(col).alias(\"max\"),\n",
    "    ).withColumn(\"column\", F.lit(col)).select(\"column\",\"non_null\",\"nulls\",\"distinct_approx\",\"mean\",\"stddev\",\"min\",\"max\"))\n",
    "\n",
    "def profile_date(df, col):\n",
    "    return (df.select(\n",
    "        F.count(F.col(col)).alias(\"non_null\"),\n",
    "        F.count(F.when(F.col(col).isNull(), col)).alias(\"nulls\"),\n",
    "        F.approx_count_distinct(col).alias(\"distinct_approx\"),\n",
    "        F.min(col).alias(\"min_date\"),\n",
    "        F.max(col).alias(\"max_date\"),\n",
    "    ).withColumn(\"column\", F.lit(col)).select(\"column\",\"non_null\",\"nulls\",\"distinct_approx\",\"min_date\",\"max_date\"))\n",
    "\n",
    "def profile_string(df, col):\n",
    "    return (df.select(\n",
    "        F.count(F.col(col)).alias(\"non_null\"),\n",
    "        F.count(F.when(F.col(col).isNull(), col)).alias(\"nulls\"),\n",
    "        F.approx_count_distinct(col).alias(\"distinct_approx\"),\n",
    "        F.min(F.length(F.col(col))).alias(\"min_len\"),\n",
    "        F.max(F.length(F.col(col))).alias(\"max_len\"),\n",
    "    ).withColumn(\"column\", F.lit(col)).select(\"column\",\"non_null\",\"nulls\",\"distinct_approx\",\"min_len\",\"max_len\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2bed91b-a5e8-4290-b85f-90fbc7e5f6d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def explore_table(fullname):\n",
    "    print(f\"\\n=== EDA: {fullname} ===\")\n",
    "    df = spark.table(fullname)\n",
    "\n",
    "    # ---- ringkasan cepat\n",
    "    rows = df.count()\n",
    "    print(\"Rows:\", rows)\n",
    "    display(df.limit(10))\n",
    "    display(spark.createDataFrame(df.dtypes, [\"column\",\"type\"]))\n",
    "\n",
    "    # ---- profil kolom per tipe\n",
    "    num_cols  = [c for c,t in df.dtypes if is_num(t)]\n",
    "    date_cols = [c for c,t in df.dtypes if is_date(t)]\n",
    "    str_cols  = [c for c,t in df.dtypes if is_str(t)]\n",
    "\n",
    "    prof_parts = []\n",
    "\n",
    "    for c in num_cols:\n",
    "        prof_parts.append(profile_numeric(df, c))\n",
    "    for c in date_cols:\n",
    "        prof_parts.append(profile_date(df, c))\n",
    "    for c in str_cols:\n",
    "        prof_parts.append(profile_string(df, c))\n",
    "\n",
    "    if prof_parts:\n",
    "        prof = reduce(lambda a,b: a.unionByName(b, allowMissingColumns=True), prof_parts)\n",
    "        display(prof)\n",
    "\n",
    "    # ---- frekuensi top untuk kolom kategorikal penting\n",
    "    for c in [\"statcode\",\"payfreq\",\"gender\",\"cnttype\",\"cntcurr\",\"pldesc01\"]:\n",
    "        if c in df.columns:\n",
    "            print(f\"\\nTop values: {c}\")\n",
    "            display(topk(df, c, 10))\n",
    "\n",
    "    # ---- cek duplikasi kunci (contoh CHDRNUM)\n",
    "    if \"chdrnum\" in df.columns:\n",
    "        dup = (df.groupBy(\"chdrnum\").count().where(\"count > 1\").orderBy(F.desc(\"count\")).limit(20))\n",
    "        print(\"\\nPotential duplicates on chdrnum:\")\n",
    "        display(dup)\n",
    "\n",
    "    # ---- validasi kode status & payfreq\n",
    "    if \"statcode\" in df.columns:\n",
    "        valid_stat = [\"IF\",\"PP\",\"LA\",\"SU\",\"RD\",\"SP\",\"TR\",\"DR\",\"DH\",\"EX\",\"FL\"]\n",
    "        anomalies = df.where(~F.upper(F.col(\"statcode\")).isin(valid_stat))\n",
    "        print(\"\\nSTATCODE anomalies (unknown):\")\n",
    "        display(anomalies.select(\"statcode\").groupBy(\"statcode\").count().orderBy(F.desc(\"count\")).limit(50))\n",
    "\n",
    "    if \"payfreq\" in df.columns:\n",
    "        valid_pf = [\"M\",\"MONTHLY\",\"Q\",\"QUARTERLY\",\"SA\",\"SEMIANNUAL\",\"H\",\"HALFYEAR\",\"A\",\"ANNUAL\",\"YEARLY\"]\n",
    "        bad_pf = df.where(~F.upper(F.col(\"payfreq\")).isin(valid_pf))\n",
    "        print(\"\\nPAYFREQ anomalies (unknown):\")\n",
    "        display(bad_pf.select(\"payfreq\").groupBy(\"payfreq\").count().orderBy(F.desc(\"count\")).limit(50))\n",
    "\n",
    "    # ---- angka negatif/aneh (premi/sum insured/cash value)\n",
    "    for c in [\"insprm\",\"annprem01\",\"sumins\",\"mscv\",\"orig_sum\"]:\n",
    "        if c in df.columns:\n",
    "            print(f\"\\nNegative or zero check: {c}\")\n",
    "            display(df.where(F.col(c) <= 0).select(c).groupBy(c).count().orderBy(c).limit(20))\n",
    "\n",
    "    # ---- tanggal anomali (urutan & range)\n",
    "    # contoh: ptdate (paid-to) tidak boleh > today + 7 hari; btdate >= hissdte\n",
    "    today = F.current_date()\n",
    "    if \"ptdate\" in df.columns:\n",
    "        print(\"\\nPTDATE future > 7 days:\")\n",
    "        display(df.where(F.col(\"ptdate\") > F.date_add(today, 7)).select(\"chdrnum\",\"ptdate\").limit(50))\n",
    "    if \"btdate\" in df.columns and \"hissdte\" in df.columns:\n",
    "        print(\"\\nBTDATE < HISSDTE (billing before issue?)\")\n",
    "        display(df.where(F.col(\"btdate\") < F.col(\"hissdte\")).select(\"chdrnum\",\"hissdte\",\"btdate\").limit(50))\n",
    "\n",
    "    # ---- whitespace/karakter aneh di ID\n",
    "    for c in [\"chdrnum\",\"clntnum\"]:\n",
    "        if c in df.columns:\n",
    "            print(f\"\\nWhitespace/odd chars in {c}:\")\n",
    "            display(df.where((F.col(c).rlike(r\"^\\s\")) | (F.col(c).rlike(r\"\\s$\")) | (F.col(c).rlike(r\"[^A-Za-z0-9_-]\")))\n",
    "                      .select(c).groupBy(c).count().orderBy(F.desc(\"count\")).limit(50))\n",
    "\n",
    "    print(\"\\n=== END EDA ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401a204a-844a-4ce9-a05d-f1a97329bf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = list_tables(CATALOG, \"01_bronze\", pattern=TABLE_PATTERN) if TABLE_PATTERN else TABLES\n",
    "for t in tables:\n",
    "    explore_table(f\"{CATALOG}.`{SCHEMA}`.{t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6641e44e-7ce1-4bd8-bf29-bcb92d8cd58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_pyspark_exploration_chandra",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
