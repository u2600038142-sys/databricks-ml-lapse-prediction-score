{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07b0638-e971-4c42-b548-5d9e7e74cd30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# UNION Bronze -> Silver (WANTED only, force STRING before union)\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# ===== CONFIG =====\n",
    "CATALOG    = \"lapse_scoring_dev\"\n",
    "SRC_SCHEMA = \"01_bronze\"\n",
    "DST_SCHEMA = \"02_silver\"\n",
    "\n",
    "TABLES = [\n",
    "    \"chandra_201701_trad_master_bronze\",\n",
    "    \"chandra_201701_ul_master_bronze\",\n",
    "    \"chandra_201702_trad_master_bronze\",\n",
    "    \"chandra_201702_ul_master_bronze\",\n",
    "    \"chandra_201703_trad_master_bronze\",\n",
    "    \"chandra_201703_ul_master_bronze\",\n",
    "    \"chandra_201704_trad_master_bronze\",\n",
    "    \"chandra_201704_ul_master_bronze\",\n",
    "    \"chandra_201705_trad_master_bronze\",\n",
    "    \"chandra_201705_ul_master_bronze\",\n",
    "    \"chandra_201706_trad_master_bronze\",\n",
    "    \"chandra_201706_ul_master_bronze\",\n",
    "    \"chandra_201707_trad_master_bronze\",\n",
    "    \"chandra_201707_ul_master_bronze\",\n",
    "    \"chandra_201708_trad_master_bronze\",\n",
    "    \"chandra_201708_ul_master_bronze\",\n",
    "    \"chandra_201709_trad_master_bronze\",\n",
    "    \"chandra_201709_ul_master_bronze\",\n",
    "    \"chandra_201710_trad_master_bronze\",\n",
    "    \"chandra_201710_ul_master_bronze\",\n",
    "    \"chandra_201711_trad_master_bronze\",\n",
    "    \"chandra_201711_ul_master_bronze\",\n",
    "    \"chandra_201712_trad_master_bronze\",\n",
    "    \"chandra_201712_ul_master_bronze\",\n",
    "    \"chandra_201801_trad_master_bronze\",\n",
    "    \"chandra_201801_ul_master_bronze\",\n",
    "    \"chandra_201802_trad_master_bronze\",\n",
    "    \"chandra_201802_ul_master_bronze\",\n",
    "    \"chandra_201803_trad_master_bronze\",\n",
    "    \"chandra_201803_ul_master_bronze\",\n",
    "    \"chandra_201804_trad_master_bronze\",\n",
    "    \"chandra_201804_ul_master_bronze\",\n",
    "    \"chandra_201805_trad_master_bronze\",\n",
    "    \"chandra_201805_ul_master_bronze\",\n",
    "    \"chandra_201806_trad_master_bronze\",\n",
    "    \"chandra_201806_ul_master_bronze\",\n",
    "    \"chandra_201807_trad_master_bronze\",\n",
    "    \"chandra_201807_ul_master_bronze\",\n",
    "    \"chandra_201808_trad_master_bronze\",\n",
    "    \"chandra_201808_ul_master_bronze\",\n",
    "    \"chandra_201809_trad_master_bronze\",\n",
    "    \"chandra_201809_ul_master_bronze\",\n",
    "    \"chandra_201810_trad_master_bronze\",\n",
    "    \"chandra_201810_ul_master_bronze\",\n",
    "    \"chandra_201811_trad_master_bronze\",\n",
    "    \"chandra_201811_ul_master_bronze\",\n",
    "    \"chandra_201812_trad_master_bronze\",\n",
    "    \"chandra_201812_ul_master_bronze\",\n",
    "    \"chandra_201901_trad_master_bronze\",\n",
    "    \"chandra_201901_ul_master_bronze\",\n",
    "    \"chandra_201902_trad_master_bronze\",\n",
    "    \"chandra_201902_ul_master_bronze\",\n",
    "    \"chandra_201903_trad_master_bronze\",\n",
    "    \"chandra_201903_ul_master_bronze\",\n",
    "    \"chandra_201904_trad_master_bronze\",\n",
    "    \"chandra_201904_ul_master_bronze\",\n",
    "    \"chandra_201905_trad_master_bronze\",\n",
    "    \"chandra_201905_ul_master_bronze\",\n",
    "    \"chandra_201906_trad_master_bronze\",\n",
    "    \"chandra_201906_ul_master_bronze\",\n",
    "    \"chandra_201907_trad_master_bronze\",\n",
    "    \"chandra_201907_ul_master_bronze\",\n",
    "    \"chandra_201908_trad_master_bronze\",\n",
    "    \"chandra_201908_ul_master_bronze\",\n",
    "    \"chandra_201909_trad_master_bronze\",\n",
    "    \"chandra_201909_ul_master_bronze\",\n",
    "    \"chandra_201910_trad_master_bronze\",\n",
    "    \"chandra_201910_ul_master_bronze\",\n",
    "    \"chandra_201911_trad_master_bronze\",\n",
    "    \"chandra_201911_ul_master_bronze\",\n",
    "    \"chandra_201912_trad_master_bronze\",\n",
    "    \"chandra_201912_ul_master_bronze\",\n",
    "]\n",
    "\n",
    "WANTED = [\n",
    "    \"chdrnum\",\"shortdesc\",\"aracde\",\"cnttype\",\"crtable\",\"hissdte\",\"subdat\",\n",
    "    \"payfreq\",\"sumins\",\"statcode\",\"lifenum\",\"age02\",\"dob02\",\"gender\",\n",
    "    \"smokeind\",\"annprem01\",\"ptdate\",\"btdate\",\"clntnum\",\n",
    "    \"age01\",\"dob01\",\"pldesc01\",\"pldesc02\"\n",
    "]\n",
    "\n",
    "TARGET_TABLE = \"chandra_union_silver\"\n",
    "WRITE_MODE   = \"overwrite\"  # gunakan overwrite supaya overwriteSchema bekerja\n",
    "\n",
    "# ===== Helpers =====\n",
    "def select_with_missing_as_null(df, wanted, default_type=\"string\"):\n",
    "    \"\"\"Select kolom sesuai urutan WANTED; kolom yg hilang diisi NULL (cast default_type).\"\"\"\n",
    "    cols = df.columns\n",
    "    sel  = [F.col(c) if c in cols else F.lit(None).cast(default_type).alias(c) for c in wanted]\n",
    "    return df.select(*sel)\n",
    "\n",
    "def force_all_string(df, cols):\n",
    "    \"\"\"Paksa semua kolom di 'cols' menjadi STRING (hindari implicit cast saat union).\"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "    return df\n",
    "\n",
    "def union_all(dfs):\n",
    "    return reduce(lambda a,b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "# ===== Ensure destination schema exists =====\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.`{DST_SCHEMA}`\")\n",
    "\n",
    "# ===== Load, align (NULL-fill + STRING), and union =====\n",
    "dfs = []\n",
    "for t in TABLES:\n",
    "    src_fqn = f\"{CATALOG}.`{SRC_SCHEMA}`.{t}\"\n",
    "    df0 = spark.table(src_fqn)\n",
    "\n",
    "    # 1) Ambil WANTED dengan kolom hilang -> NULL\n",
    "    dfi = select_with_missing_as_null(df0, WANTED, default_type=\"string\")\n",
    "\n",
    "    # 2) ðŸ”‘ Paksa SEMUA kolom WANTED menjadi STRING sebelum union (penting!)\n",
    "    dfi = force_all_string(dfi, WANTED)\n",
    "\n",
    "    # 3) Lineage\n",
    "    dfi = dfi.withColumn(\"_source_table\", F.lit(t)) \\\n",
    "             .withColumn(\"_ingest_ts_union\", F.current_timestamp())\n",
    "\n",
    "    dfs.append(dfi)\n",
    "\n",
    "# Union (sekarang aman karena tipe sudah seragam STRING)\n",
    "df_union = dfs[0] if len(dfs) == 1 else union_all(dfs)\n",
    "\n",
    "# ===== Write to Silver (schema STRING, overwrite) =====\n",
    "TARGET_FQN = f\"{CATALOG}.`{DST_SCHEMA}`.{TARGET_TABLE}\"\n",
    "\n",
    "(df_union.write\n",
    "    .mode(WRITE_MODE)                 # pakai overwrite agar schema di-redefine\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"overwriteSchema\", \"true\")  # pastikan schema target mengikuti df_union (semua STRING)\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(TARGET_FQN))\n",
    "\n",
    "print(f\"Write OK -> {TARGET_FQN}\")\n",
    "print(\"Rows:\", spark.table(TARGET_FQN).count())\n",
    "display(spark.sql(f\"SELECT * FROM {TARGET_FQN} LIMIT 50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec804f0-73c0-46e6-a335-fc775dff5a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Rename columns to business names (make a new Silver table)\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG     = \"lapse_scoring_dev\"\n",
    "SRC_SCHEMA  = \"02_silver\"\n",
    "SRC_TABLE   = \"chandra_union_silver\"          # tabel hasil union (semua STRING)\n",
    "DST_SCHEMA  = \"02_silver\"\n",
    "DST_TABLE   = \"chandra_union_bn_silver\"       # bn = business names\n",
    "\n",
    "# Mapping: old -> new (edit jika perlu)\n",
    "BUSINESS_MAP = {\n",
    "    \"chdrnum\":   \"Contract_Number\",\n",
    "    \"shortdesc\": \"Product_Category\",\n",
    "    \"aracde\":    \"Product_Group\",\n",
    "    \"cnttype\":   \"Product\",\n",
    "    \"crtable\":   \"Product_Detail\",\n",
    "    \"hissdte\":   \"Policy_Issue_Date\",\n",
    "    \"subdat\":    \"Submission_Date\",\n",
    "    \"payfreq\":   \"Payment_Frequency\",\n",
    "    \"sumins\":    \"Sum_Assured\",\n",
    "    \"statcode\":  \"Policy_Status_Code\",\n",
    "    \"lifenum\":   \"Life_Number\",\n",
    "    \"age02\":     \"Age_of_the_Life_Insured\",\n",
    "    \"dob02\":     \"Date_of_Birth_of_the_Life_Insured\",\n",
    "    \"gender\":    \"Gender_of_the_Life_Insured\",\n",
    "    \"smokeind\":  \"Smoking_Indicator\",\n",
    "    \"annprem01\": \"Annualized_Premium\",\n",
    "    \"ptdate\":    \"Paid_to_Date\",\n",
    "    \"btdate\":    \"Billing_to_Date\",\n",
    "    \"clntnum\":   \"Client_Number\",\n",
    "    \"age01\":     \"Age_of_the_Policy_Owner\",\n",
    "    \"dob01\":     \"Date_of_Birth_of_the_Policy_Owner\",\n",
    "    \"pldesc01\":  \"Name_of_the_Policy_Owner\",\n",
    "    \"pldesc02\":  \"Name_of_the_Life_Insured\",\n",
    "    # lineage\n",
    "    \"_source_table\":     \"source_table\",\n",
    "    \"_ingest_ts_union\":  \"union_ingest_ts\",\n",
    "}\n",
    "\n",
    "SRC_FQN = f\"{CATALOG}.`{SRC_SCHEMA}`.{SRC_TABLE}\"\n",
    "DST_FQN = f\"{CATALOG}.`{DST_SCHEMA}`.{DST_TABLE}\"\n",
    "\n",
    "df = spark.table(SRC_FQN)\n",
    "\n",
    "# cek kolom yang ada vs mapping\n",
    "existing = set(df.columns)\n",
    "mapped_existing = {k:v for k,v in BUSINESS_MAP.items() if k in existing}\n",
    "missing_in_source = [k for k in BUSINESS_MAP.keys() if k not in existing]\n",
    "if missing_in_source:\n",
    "    print(\"âš ï¸ Missing in source (ignored):\", missing_in_source)\n",
    "\n",
    "# select dengan alias business\n",
    "df_bn = df.select([F.col(k).alias(v) for k,v in mapped_existing.items()])\n",
    "\n",
    "# tulis sebagai tabel baru\n",
    "(df_bn.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\",\"true\")\n",
    "    .option(\"overwriteSchema\",\"true\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(DST_FQN))\n",
    "\n",
    "print(f\"Write OK -> {DST_FQN}\")\n",
    "display(spark.sql(f\"SELECT * FROM {DST_FQN} LIMIT 50\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f511446-2f38-4be8-8a59-042d4c68fd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CAST tipe data dari BN -> simpan ke tabel BARU\n",
    "# ============================================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- CONFIG ---\n",
    "CATALOG      = \"lapse_scoring_dev\"\n",
    "SRC_SCHEMA   = \"02_silver\"\n",
    "SRC_TABLE    = \"chandra_union_bn_silver\"        # tabel BN sumber (string-heavy)\n",
    "DST_SCHEMA   = \"02_silver\"\n",
    "DST_TABLE    = \"chandra_union_dt_silver\"  # >>> tabel BARU hasil casting\n",
    "\n",
    "SRC_FQN = f\"{CATALOG}.`{SRC_SCHEMA}`.{SRC_TABLE}\"\n",
    "DST_FQN = f\"{CATALOG}.`{DST_SCHEMA}`.{DST_TABLE}\"\n",
    "\n",
    "# Pastikan schema tujuan ada\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.`{DST_SCHEMA}`\")\n",
    "\n",
    "# --- LOAD ---\n",
    "df = spark.table(SRC_FQN)\n",
    "\n",
    "# 1) Trim semua kolom string\n",
    "for c, t in df.dtypes:\n",
    "    if t.startswith(\"string\"):\n",
    "        df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "# 2) Uppercase untuk kode/flag (sesuaikan jika ada tambahan)\n",
    "uppercase_cols = [\n",
    "    \"Policy_Status_Code\",\n",
    "    \"Payment_Frequency\",\n",
    "    \"Gender_of_the_Life_Insured\",\n",
    "    \"Smoking_Indicator\",\n",
    "]\n",
    "for c in uppercase_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.upper(F.col(c)))\n",
    "\n",
    "# 3) Parse TANGGAL: seluruh data berformat yyyyMMdd (tolerant â†’ NULL jika gagal)\n",
    "date_cols = [\n",
    "    \"Policy_Issue_Date\",\n",
    "    \"Submission_Date\",\n",
    "    \"Paid_to_Date\",\n",
    "    \"Billing_to_Date\",\n",
    "    \"Date_of_Birth_of_the_Policy_Owner\",\n",
    "    \"Date_of_Birth_of_the_Life_Insured\",\n",
    "]\n",
    "for c in date_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            F.expr(f\"try_to_date(regexp_replace(CAST({c} AS STRING), '[^0-9]', ''), 'yyyyMMdd')\")\n",
    "        )\n",
    "\n",
    "# 4) Numerik (double): bersihkan locale lalu cast ke double\n",
    "def to_double(col):\n",
    "    s = F.regexp_replace(col.cast(\"string\"), r\"\\s\", \"\")\n",
    "    return (\n",
    "        F.when(s.rlike(r\"^\\d{1,3}(\\.\\d{3})+,\\d+$\"), F.regexp_replace(F.regexp_replace(s, r\"\\.\", \"\"), \",\", \".\"))\n",
    "         .when(s.rlike(r\"^\\d+,\\d+$\"),               F.regexp_replace(s, \",\", \".\"))\n",
    "         .when(s.rlike(r\"^\\d{1,3}(,\\d{3})+\\.\\d+$\"), F.regexp_replace(s, \",\", \"\"))\n",
    "         .otherwise(s)\n",
    "    ).cast(\"double\")\n",
    "\n",
    "double_cols = [\n",
    "    \"Sum_Assured\",\n",
    "    \"Annualized_Premium\",\n",
    "    # Tambahkan kolom numeric lain jika ada, mis.:\n",
    "    # \"Loan_Value\", \"Total_Fee\", \"Coverage_Debt\"\n",
    "]\n",
    "for c in double_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, to_double(F.col(c)))\n",
    "\n",
    "# 5) Integer: usia (Life_Number dibiarkan string jika itu ID)\n",
    "int_cols = [\n",
    "    \"Age_of_the_Policy_Owner\",\n",
    "    \"Age_of_the_Life_Insured\",\n",
    "]\n",
    "for c in int_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(\"int\"))\n",
    "\n",
    "# 6) Pastikan ID/teks utama tetap STRING (defensif)\n",
    "id_string_cols = [\n",
    "    \"Contract_Number\", \"Client_Number\", \"Life_Number\",\n",
    "    \"Product_Category\", \"Product_Group\", \"Product\", \"Product_Detail\",\n",
    "    \"Name_of_the_Policy_Owner\", \"Name_of_the_Life_Insured\",\n",
    "    \"source_table\"\n",
    "]\n",
    "for c in id_string_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "# 7) Tulis ke TABEL BARU (tidak menyentuh tabel sumber)\n",
    "(df.write\n",
    "    .mode(\"overwrite\")              # buat/replace tabel baru\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(DST_FQN))\n",
    "\n",
    "print(f\"Typed table written -> {DST_FQN}\")\n",
    "print(\"Rows:\", spark.table(DST_FQN).count())\n",
    "display(spark.sql(f\"DESCRIBE {DST_FQN}\"))\n",
    "display(spark.sql(f\"SELECT * FROM {DST_FQN} LIMIT 20\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bfc9f93-3e13-49b4-b150-0731393baf4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_pyspark_union_chandra",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
